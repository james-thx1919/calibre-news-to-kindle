#!/usr/bin/env python
from calibre.web.feeds.news import BasicNewsRecipe

class GlobeAndMail(BasicNewsRecipe):
    title          = u'The Globe and Mail'
    oldest_article = 2
    max_articles_per_feed = 50
    auto_cleanup   = True
    language       = 'en_CA'
    __author__     = 'Kovid Goyal/Modified for Subscription'

    # --- Login Credentials ---
    needs_subscription = True
    requires_password = True
    # Using the direct registration/login path which is more stable for robots
    login_url = 'https://www.theglobeandmail.com/real-time/registration/login/'

    # This prevents the Globe from seeing Calibre as a bot
    browser_types = ['chrome']
    get_browser_user_agent = lambda x: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'

    def get_browser(self):
        br = BasicNewsRecipe.get_browser(self)
        if self.username and self.password:
            try:
                self.log("Attempting to login to The Globe and Mail...")
                br.open(self.login_url)
                
                # Smart Form Selection: Find the form that actually has an email field
                def select_login_form(form):
                    return "email" in [str(c.name) for c in form.controls]
                
                br.select_form(predicate=select_login_form)
                br['email'] = self.username
                br['password'] = self.password
                br.submit()
                self.log("Login form submitted.")
            except Exception as e:
                self.log.error("Login failed: %s" % str(e))
        return br

    def parse_index(self):
        sections = [
            ('Canada', 'https://www.theglobeandmail.com/canada/'),
            ('Politics', 'https://www.theglobeandmail.com/politics/'),
            ('Business', 'https://www.theglobeandmail.com/business/'),
            ('World', 'https://www.theglobeandmail.com/world/'),
            ('Opinion', 'https://www.theglobeandmail.com/opinion/')
        ]
        
        ans = []
        for section_title, url in sections:
            articles = []
            soup = self.index_to_soup(url)
            # This looks for standard links in the main content areas
            for a in soup.findAll('a', href=True):
                # Only grab links that look like articles
                if '/article-' in a['href'] or (a['href'].startswith('/') and len(a['href']) > 30):
                    title = self.tag_to_string(a).strip()
                    if title and len(title) > 10:
                        url = 'https://www.theglobeandmail.com' + a['href'] if a['href'].startswith('/') else a['href']
                        articles.append({'title': title, 'url': url})
            
            if articles:
                # Remove duplicates while preserving order
                seen = set()
                final_articles = []
                for art in articles:
                    if art['url'] not in seen:
                        final_articles.append(art)
                        seen.add(art['url'])
                ans.append((section_title, final_articles))
        return ans
